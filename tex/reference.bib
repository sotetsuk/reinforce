% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Haarnoja2017-xl,
  title         = "Reinforcement Learning with Deep {Energy-Based} Policies",
  author        = "Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and
                   Levine, Sergey",
  abstract      = "We propose a method for learning expressive energy-based
                   policies for continuous states and actions, which has been
                   feasible only in tabular domains before. We apply our method
                   to learning maximum entropy policies, resulting into a new
                   algorithm, called soft Q-learning, that expresses the
                   optimal policy via a Boltzmann distribution. We use the
                   recently proposed amortized Stein variational gradient
                   descent to learn a stochastic sampling network that
                   approximates samples from this distribution. The benefits of
                   the proposed algorithm include improved exploration and
                   compositionality that allows transferring skills between
                   tasks, which we confirm in simulated experiments with
                   swimming and walking robots. We also draw a connection to
                   actor-critic methods, which can be viewed performing
                   approximate inference on the corresponding energy-based
                   model.",
  month         =  feb,
  year          =  2017,
  keywords      = "REINFORCE",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1702.08165"
}

@PHDTHESIS{Parmas2020-tr,
  title    = "Total stochastic gradient algorithms and applications to
              model-based reinforcement learning",
  author   = "Parmas, Paavo",
  abstract = "CMS,Netcommons,Maple",
  month    =  feb,
  year     =  2020,
  keywords = "REINFORCE"
}

@BOOK{Sutton2018-ij,
  title     = "Reinforcement Learning, second edition: An Introduction",
  author    = "Sutton, Richard S and Barto, Andrew G",
  abstract  = "The significantly expanded and updated new edition of a widely
               used text on reinforcement learning, one of the most active
               research areas in artificial intelligence.Reinforcement
               learning, one of the most active research areas in artificial
               intelligence, is a computational approach to learning whereby an
               agent tries to maximize the total amount of reward it receives
               while interacting with a complex, uncertain environment. In
               Reinforcement Learning, Richard Sutton and Andrew Barto provide
               a clear and simple account of the field's key ideas and
               algorithms. This second edition has been significantly expanded
               and updated, presenting new topics and updating coverage of
               other topics.Like the first edition, this second edition focuses
               on core online learning algorithms, with the more mathematical
               material set off in shaded boxes. Part I covers as much of
               reinforcement learning as possible without going beyond the
               tabular case for which exact solutions can be found. Many
               algorithms presented in this part are new to the second edition,
               including UCB, Expected Sarsa, and Double Learning. Part II
               extends these ideas to function approximation, with new sections
               on such topics as artificial neural networks and the Fourier
               basis, and offers expanded treatment of off-policy learning and
               policy-gradient methods. Part III has new chapters on
               reinforcement learning's relationships to psychology and
               neuroscience, as well as an updated case-studies chapter
               including AlphaGo and AlphaGo Zero, Atari game playing, and IBM
               Watson's wagering strategy. The final chapter discusses the
               future societal impacts of reinforcement learning.",
  publisher = "MIT Press",
  month     =  nov,
  year      =  2018,
  keywords  = "RL;imbalanced-rl;mjx-paper;REINFORCE",
  language  = "en"
}

@ARTICLE{Williams1992-rp,
  title    = "Simple statistical gradient-following algorithms for
              connectionist reinforcement learning",
  author   = "Williams, Ronald J",
  abstract = "This article presents a general class of associative
              reinforcement learning algorithms for connectionist networks
              containing stochastic units. These algorithms, called REINFORCE
              algorithms, are shown to make weight adjustments in a direction
              that lies along the gradient of expected reinforcement in both
              immediate-reinforcement tasks and certain limited forms of
              delayed-reinforcement tasks, and they do this without explicitly
              computing gradient estimates or even storing information from
              which such estimates could be computed. Specific examples of such
              algorithms are presented, some of which bear a close relationship
              to certain existing algorithms while others are novel but
              potentially interesting in their own right. Also given are
              results that show how such algorithms can be naturally integrated
              with backpropagation. We close with a brief discussion of a
              number of additional issues surrounding the use of such
              algorithms, including what is known about their limiting
              behaviors as well as further considerations that might be used to
              help develop similar but potentially more powerful reinforcement
              learning algorithms.",
  journal  = "Mach. Learn.",
  volume   =  8,
  number   =  3,
  pages    = "229--256",
  month    =  may,
  year     =  1992,
  keywords = "Mahjong;RL;REINFORCE"
}
