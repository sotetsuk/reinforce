\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{natbib}         % bibliography \usepackage{apalike}
\usepackage{xcolor}
\usepackage{graphicx}

\title{REINFORCE Variants}

\author{
Sotetsu KOYAMADA
}

\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
We summarize the famous REINFORCE~\citep{Williams1992-rp} algorithm and its variants.
\end{abstract}

\section{Notation}

\begin{itemize}
	\item Assumes Episodic MDPs.
	\item Discount factor is omitted for the simplicity.
\end{itemize}

The reinforcement learning objective is defined by:

\begin{eqnarray}
J(\theta) := \mathbb{E}_{(s_1, a_1, \ldots, s_T, a_T) \sim p_\theta(s_1, a_1, \ldots, s_T, a_T)} \Biggl[ \sum_{t=1}^{T} r(s_t, a_t) \Biggr].
\end{eqnarray}

\section{REINFORCE Variants}

Policy gradient $\nabla_\theta J(\theta)$ is derived as

\begin{eqnarray}
\nabla_\theta J(\theta)
= \mathbb{E}_{(s_1, a_1, \ldots, s_T, a_T) \sim p_\theta(s_1, a_1, \ldots, s_T, a_T)} \Biggl[ \Biggl(\sum_{t^\prime=1}^{T} \nabla_\theta \log \pi_\theta (a_{t^\prime}|s_{t^\prime}) \Biggr) \Biggl( \sum_{t=1}^T r(s_t, a_t) \Biggr) \Biggr].
\end{eqnarray}

\subsection{Vanilla REINFORCE}

\begin{eqnarray}
\nabla_\theta J(\theta)
&\approx& \frac{1}{N} \sum_{n=1}^{N} \Biggl( \sum_{t^\prime=1}^{T} \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime}) \Biggr) \Biggl( \sum_{t=1}^T r(s_{n, t}, a_{n, t}) \Biggr) \\
&=& \frac{1}{N} \sum_{n=1}^{N} \mathcal{R}_n \sum_{t^\prime=1}^{T} \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}
where $\mathcal{R}_{n} := \sum_{t=1}^T r(s_{n, t}, a_{n, t})$.

\subsection{Future Return}

\begin{eqnarray}
\nabla_\theta J(\theta)
&\approx& \frac{1}{N} \sum_{n=1}^{N} \Biggl( \sum_{t^\prime=1}^{T} \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime}) \Biggl(\, \sum_{{\color{red}{t=t^\prime}}}^{T} r(s_{n, t}, a_{n, t}) \Biggr) \Biggr)  \\
&=& \frac{1}{N} \sum_{n=1}^{N} \sum_{t^\prime=1}^{T} \mathcal{R}_{n, {\color{red}{t^\prime}}} \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}
where $\mathcal{R}_{n, t^\prime} := \sum_{t=t^\prime}^T r(s_{n, t}, a_{n, t})$.


\subsection{Maximum Entropy}
Let us define maximum entropy RL objective \citep{Haarnoja2017-xl} (TODO: check other ref) by
\begin{eqnarray}
J(\theta) := \mathbb{E}_{(s_1, a_1, \ldots, s_T, a_T) \sim p_\theta(s_1, a_1, \ldots, s_T, a_T)} \Biggl[ \sum_{t=1}^{T} r(s_t, a_t) {\color{red}{ + \alpha \mathcal{H} \bigl(\pi_\theta(\, \cdot \,|\, s_t) \bigr) }}\Biggr],
\end{eqnarray}
where $\mathcal{H} \bigl(\pi_\theta(\, \cdot \,|\, s_t) \bigr)$ is the entropy term of policy $\pi_\theta$ at state $s_t$ and $\alpha$ is a hyperparameter.
Then, the gradient is derived as
\begin{eqnarray}
\nabla_\theta J(\theta)
\approx \frac{1}{N} \sum_{n=1}^{N} {\color{red}{\mathcal{R}_n^{\mathrm{Ent}}}} \sum_{t^\prime=1}^{T} \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}
where $\mathcal{R}_{n}^{\mathrm{Ent}} := \sum_{t=1}^T r(s_{n, t}, a_{n, t}) {\color{red}{ + \alpha \mathcal{H} \bigl(\pi_\theta(\, \cdot \,|\, s_{n, t}) \bigr) }}$.
For the future return case,
\begin{eqnarray}
\nabla_\theta J(\theta)
\approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t^\prime=1}^{T} {\color{red}{\mathcal{R}_{n, t^\prime}^{\mathrm{Ent}}}} \, \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}
where $\mathcal{R}_{n, t^\prime}^{\mathrm{Ent}} := \sum_{t=t^\prime}^T r(s_{n, t}, a_{n, t}) {\color{red}{ + \alpha \mathcal{H} \bigl(\pi_\theta(\, \cdot \,|\, s_{n, t}) \bigr) }}$.


\subsection{Average Baselines}

\begin{eqnarray}
\nabla_\theta J(\theta)
\approx \frac{1}{N} \sum_{n=1}^{N} \Biggl(\mathcal{R}_n {\color{red}{- \frac{1}{N} \sum_{n^\prime=1}^N \mathcal{R}_n }} \Biggr)\sum_{t^\prime=1}^{T} \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}

For the future return case,

\begin{eqnarray}
\nabla_\theta J(\theta)
\approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t^\prime=1}^{T} \Biggl(\mathcal{R}_{n, t^\prime} {\color{red}{- \frac{1}{N} \sum_{n^\prime=1}^N \mathcal{R}_{n^\prime, t^\prime} }} \Biggr)  \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}

\subsubsection{Debiasing Factor}
While it is well known that introducing action-independent baseline is unbiased,
estimating the baseline using the same samples used in return estimation may introduce some bias.
Rescaling the gradient can make the estimator unbiased.

\begin{eqnarray}
\nabla_\theta J(\theta)
\approx \frac{1}{{\color{red}{N-1}}} \sum_{n=1}^{N} \Biggl(\mathcal{R}_n - \frac{1}{N} \sum_{n^\prime=1}^N \mathcal{R}_n \Biggr)\sum_{t^\prime=1}^{T} \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}
See Section 2.4 in \citet{Parmas2020-tr} for the detailed explanation.
For the future return case,
\begin{eqnarray}
\nabla_\theta J(\theta)
\approx \frac{1}{{\color{red}{N-1}}} \sum_{n=1}^{N} \sum_{t^\prime=1}^{T} \Biggl(\mathcal{R}_{n, t^\prime} - \frac{1}{N} \sum_{n^\prime=1}^N \mathcal{R}_{n^\prime, t^\prime} \Biggr)  \nabla_\theta \log \pi_\theta (a_{n, t^\prime}|s_{n, t^\prime})
\end{eqnarray}

\bibliographystyle{apalike}
\bibliography{reference}

\end{document}
